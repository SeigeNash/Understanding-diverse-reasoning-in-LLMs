{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "582e7945",
   "metadata": {},
   "source": [
    "# MMR calculation on student completions "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f45bdb7",
   "metadata": {},
   "source": [
    "For Flan T5 base model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "115d9f16",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk \n",
    "import json \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from rouge_score import rouge_scorer\n",
    "import torch\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, BertTokenizer, BertModel\n",
    "from transformers import BertTokenizer, BertModel\n",
    "\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "\n",
    "import copy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7acd06ba",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "13047d43",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_rougeL(text1, text2):\n",
    "    scorer = rouge_scorer.RougeScorer(['rougeL'], use_stemmer=True)\n",
    "    scores = scorer.score(text1, text2)\n",
    "    return scores\n",
    "\n",
    "def calculate_rouge2(text1, text2):\n",
    "    scorer = rouge_scorer.RougeScorer(['rouge2'], use_stemmer=True)\n",
    "    scores = scorer.score(text1, text2)\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4fc2fc62",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_bertscore(text1, text2, tokenizer, model):\n",
    "    inputs = tokenizer([text1, text2], return_tensors='pt', padding=True, truncation=True)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "    \n",
    "    embeddings = outputs.last_hidden_state\n",
    "    # Calculate cosine similarity between the embeddings of text1 and text2\n",
    "    similarity = torch.nn.functional.cosine_similarity(embeddings[0], embeddings[1], dim=0).mean().item()\n",
    "    return similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "689e0f83",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_json(file_path):\n",
    "    with open(file_path, 'r') as file:\n",
    "        data = json.load(file)\n",
    "        return data  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6dc2e2f",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b874e45f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# following function makes a list of list containing all the rationales with multiple reasonings\n",
    "\n",
    "def correct_rationale_dict(ds_dict):\n",
    "    correct_list = []\n",
    "    ds_data = ds_dict['data']\n",
    "    for key in ds_data.keys():\n",
    "        if len(ds_data[key]) > 1 :\n",
    "            l = []\n",
    "            for i in ds_data[key]:\n",
    "                # this is within the list corresponding to each index in data dictionary\n",
    "                # i is a dictionary \n",
    "                if i['answer'] in i['completion']:\n",
    "                    l.append(i)\n",
    "            correct_list.append(l)\n",
    "    return correct_list\n",
    "\n",
    "\n",
    "def formatTeacherDataset(teacher_list):\n",
    "    for i in range(len(teacher_list)):\n",
    "        if len(teacher_list[i]) == 0:\n",
    "            continue\n",
    "        d1 = dict()\n",
    "        d1['sample_index'] = teacher_list[i][0]['sample_index']\n",
    "        d1['question'] = teacher_list[i][0]['question']\n",
    "        d1['rationale_dict'] = list()\n",
    "        for j in range(len(teacher_list[i])):\n",
    "            ci = teacher_list[i][j]['completion_index']\n",
    "            rationale = teacher_list[i][j]['reasoning_completion']\n",
    "            l = list()\n",
    "            l.append(ci)\n",
    "            l.append(rationale)\n",
    "            d1['rationale_dict'].append(l)\n",
    "        teacher_list[i] = d1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9917edf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2d81f61d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "The following modules are for preprocessing the student dataset in a way that it becomes useful \n",
    "\"\"\"\n",
    "\n",
    "# make a function for removing all the student rationales that have been incorrectly generated \n",
    "\n",
    "def removeIncorrectStudentRat(student_dataset):\n",
    "    i = 0\n",
    "    while(i != len(student_dataset)):\n",
    "        if( student_dataset[i]['correct'] == False ):\n",
    "            d = student_dataset[i]\n",
    "            student_dataset.remove(d)\n",
    "        else:\n",
    "            i += 1\n",
    "             \n",
    "\n",
    "# The followinf function compares all the student rationales and find their respective sample indices in the dataset \n",
    "\n",
    "def addSI(student_list, teacher_list):\n",
    "    for i in range(len(teacher_list)):\n",
    "        if len(teacher_list[i]) == 0:\n",
    "            continue\n",
    "        question = teacher_list[i]['question']\n",
    "        si = teacher_list[i]['sample_index']     \n",
    "        for j in range(len(student_list)):\n",
    "            prompt = student_list[j]['prompt']\n",
    "            if( question in prompt ):\n",
    "                student_list[j]['sample_index'] = si\n",
    "                break            \n",
    "                \n",
    "# removing all the rationales with no sample_indices in them from student dataset \n",
    "\n",
    "def helper(student_dataset):\n",
    "    i = 0 \n",
    "    while( i != len(student_dataset)):\n",
    "        if ( 'sample_index' not in student_dataset[i] ):\n",
    "            d = student_dataset[i]\n",
    "            student_dataset.remove(d)\n",
    "        else:\n",
    "            i += 1\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "597e602a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# the following function removes rationales from the teacher dataset \n",
    "\n",
    "def removeEmpty(teacher_list):\n",
    "    i = 0\n",
    "    while( i != len(teacher_list)):\n",
    "        if (len(teacher_list[i]) != 0 ):\n",
    "            i += 1\n",
    "        else:\n",
    "            d = teacher_list[i]\n",
    "            teacher_list.remove(d)             \n",
    "            \n",
    "\n",
    "def removeRat(student_list, teacher_list):\n",
    "    # Create a set of sample indices from student_list for fast lookup\n",
    "    student_indices = {student['sample_index'] for student in student_list}\n",
    "    \n",
    "    # Use list comprehension to create a new list with only the desired elements\n",
    "    teacher_list = [d for d in teacher_list if 'sample_index' in d and (len(d) != 0) and (d['sample_index'] in student_indices)]\n",
    "    \n",
    "    return teacher_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fef79778",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\ndef mmr1(student_prompt, teacher_rationale_dict, lambda_const):\\n\\n    ranking_set = []\\n    rationales = copy.deepcopy(teacher_rationale_dict)\\n    while len(r) > 0:\\n    \\n        rationale_to_add = list()\\n        score = 0\\n        rouge_score_to_add = 0\\n        \\n        for rationale in teacher_rationale_dict:\\n        \\n            first_part = calculate_rouge(student_prompt, rationale[1])\\n            first_part = first_part['rouge2'].fmeasure\\n            second_part = 0\\n            \\n            for di in ranking_set:\\n            \\n                sim = calculate_rouge(di[1], rationale[1])\\n                sim = sim['rouge2'].fmeasure\\n                \\n                if sim > second_part:\\n                    second_part = sim\\n                    \\n            mmr_score = lambda_const*(first_part)-(1-lambda_const) * second_part\\n            \\n            if (mmr_score > score):\\n                score = mmr_score\\n                rationale_to_add = rationale\\n                rouge_score_to_add = first_part\\n                \\n        teacher_rationale_dict.remove(rationale_to_add)\\n        rationale_to_add.append(rouge_score_to_add)\\n        ranking_set.append(rationale_to_add)\\n        \\n    return ranking_set\\n\""
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def mmr(student_prompt, teacher_rationale_list, lambda_const, metrics):\n",
    "    ranking_set = []\n",
    "    \n",
    "    if( metrics == 'bert'):\n",
    "        bert_tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "        bert_model = BertModel.from_pretrained('bert-base-uncased') \n",
    "    \n",
    "    while len(teacher_rationale_list) > 0:\n",
    "        rationale_to_add = None\n",
    "        score = 0\n",
    "        rouge_score_to_add = 0\n",
    "        \n",
    "        for rationale in teacher_rationale_list:\n",
    "            \n",
    "            if( metrics == 'bert'):\n",
    "                first_part = calculate_bertscore(student_prompt, rationale[1], bert_tokenizer, bert_model)\n",
    "            elif (metrics == 'rougeL'):\n",
    "                first_part = calculate_rougeL(student_prompt, rationale[1])\n",
    "                first_part = first_part['rougeL'].fmeasure\n",
    "            elif (metrics == 'rouge2'):\n",
    "                first_part = calculate_rouge2(student_prompt, rationale[1])\n",
    "                first_part = first_part['rouge2'].fmeasure\n",
    "            else:\n",
    "                print(\"Incorrect metric selection\")\n",
    "                break\n",
    "                \n",
    "            second_part = 0\n",
    "            \n",
    "            for di in ranking_set:\n",
    "                \n",
    "                if( metrics == 'bert'):\n",
    "                    sim = calculate_bertscore(di[1], rationale[1], bert_tokenizer, bert_model)\n",
    "                elif (metrics == 'rougeL'):\n",
    "                    sim = calculate_rougeL(di[1], rationale[1])\n",
    "                    sim = sim['rougeL'].fmeasure\n",
    "                elif (metrics == 'rouge2'):\n",
    "                    sim = calculate_rouge2(di[1], rationale[1])\n",
    "                    sim = sim['rouge2'].fmeasure\n",
    "                else:\n",
    "                    print(\"Incorrect metric selection\")\n",
    "                    break\n",
    "                \n",
    "                if sim > second_part:\n",
    "                    second_part = sim\n",
    "            \n",
    "            mmr_score = lambda_const*(first_part)-(1-lambda_const) * second_part\n",
    "            if (mmr_score > score):\n",
    "                score = mmr_score\n",
    "                rationale_to_add = rationale\n",
    "                rouge_score_to_add = first_part\n",
    "        \n",
    "        if rationale_to_add is not None:\n",
    "            # print(f\"Removing rationale: {rationale_to_add}\")\n",
    "            teacher_rationale_list.remove(rationale_to_add)\n",
    "            rationale_to_add.append(rouge_score_to_add)\n",
    "            ranking_set.append(rationale_to_add)\n",
    "        else:\n",
    "            # print(\"No rationale to add found.\")\n",
    "            break\n",
    "                \n",
    "        #teacher_rationale_list.remove(rationale_to_add)\n",
    "        #rationale_to_add.append(rouge_score_to_add)\n",
    "        #ranking_set.append(rationale_to_add)\n",
    "    \n",
    "    return ranking_set\n",
    "        \n",
    "\n",
    "\n",
    "\n",
    "# understand your mistakes from the following \n",
    "\"\"\"\n",
    "def mmr1(student_prompt, teacher_rationale_dict, lambda_const):\n",
    "\n",
    "    ranking_set = []\n",
    "    rationales = copy.deepcopy(teacher_rationale_dict)\n",
    "    while len(r) > 0:\n",
    "    \n",
    "        rationale_to_add = list()\n",
    "        score = 0\n",
    "        rouge_score_to_add = 0\n",
    "        \n",
    "        for rationale in teacher_rationale_dict:\n",
    "        \n",
    "            first_part = calculate_rouge(student_prompt, rationale[1])\n",
    "            first_part = first_part['rouge2'].fmeasure\n",
    "            second_part = 0\n",
    "            \n",
    "            for di in ranking_set:\n",
    "            \n",
    "                sim = calculate_rouge(di[1], rationale[1])\n",
    "                sim = sim['rouge2'].fmeasure\n",
    "                \n",
    "                if sim > second_part:\n",
    "                    second_part = sim\n",
    "                    \n",
    "            mmr_score = lambda_const*(first_part)-(1-lambda_const) * second_part\n",
    "            \n",
    "            if (mmr_score > score):\n",
    "                score = mmr_score\n",
    "                rationale_to_add = rationale\n",
    "                rouge_score_to_add = first_part\n",
    "                \n",
    "        teacher_rationale_dict.remove(rationale_to_add)\n",
    "        rationale_to_add.append(rouge_score_to_add)\n",
    "        ranking_set.append(rationale_to_add)\n",
    "        \n",
    "    return ranking_set\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "15c7f8aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def MMRcaculation(teacher_list, student_list, lambda_const, metrics):\n",
    "    # this function passes the rationale's dictionary from teacher_dataset and prompt to calculate MMR\n",
    "    mmr_list = []\n",
    "    for i in range(len(teacher_list)):\n",
    "        if len(teacher_list[i]) == 0:\n",
    "            continue\n",
    "            \n",
    "        teacher_rationale_list = copy.deepcopy(teacher_list[i]['rationale_dict'])\n",
    "        \n",
    "        s = dict()\n",
    "        prompt = teacher_list[i]['question'] + student_list[i]['model_inference']\n",
    "        s['sample_index'] = teacher_list[i]['sample_index']\n",
    "        s['prompt'] = prompt\n",
    "        s['ranking'] = mmr(prompt, teacher_rationale_list, lambda_const, metrics)\n",
    "        mmr_list.append(s)\n",
    "    return mmr_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2887d801",
   "metadata": {},
   "outputs": [],
   "source": [
    "def makeJSON(data_list, file_name):\n",
    "    with open(file_name, 'w') as json_file:\n",
    "        json.dump(data_list, json_file, indent=4)\n",
    "    print(f\"Data has been written to {file_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1f245ab1",
   "metadata": {},
   "outputs": [],
   "source": [
    "addsub70_path = '/Users/shiprasingh/IIT KGP internship /reasoning-teacher/saved/teacher_completion_data/B_text-davinci-002__C_zs_cot_t70/D_addsub.json'\n",
    "coin70_path = '/Users/shiprasingh/IIT KGP internship /reasoning-teacher/saved/teacher_completion_data/B_text-davinci-002__C_zs_cot_t70/D_coin_flip.json'\n",
    "du70_path = '/Users/shiprasingh/IIT KGP internship /reasoning-teacher/saved/teacher_completion_data/B_text-davinci-002__C_zs_cot_t70/D_date_understanding.json'\n",
    "llconc70_path = '/Users/shiprasingh/IIT KGP internship /reasoning-teacher/saved/teacher_completion_data/B_text-davinci-002__C_zs_cot_t70/D_last_letter_concatenation.json'\n",
    "mulAr70_path = '/Users/shiprasingh/IIT KGP internship /reasoning-teacher/saved/teacher_completion_data/B_text-davinci-002__C_zs_cot_t70/D_multiarith.json'\n",
    "seq70_path = '/Users/shiprasingh/IIT KGP internship /reasoning-teacher/saved/teacher_completion_data/B_text-davinci-002__C_zs_cot_t70/D_single_eq.json'\n",
    "strat70_path = '/Users/shiprasingh/IIT KGP internship /reasoning-teacher/saved/teacher_completion_data/B_text-davinci-002__C_zs_cot_t70/D_strategy_qa.json'\n",
    "svamp70_path = '/Users/shiprasingh/IIT KGP internship /reasoning-teacher/saved/teacher_completion_data/B_text-davinci-002__C_zs_cot_t70/D_svamp.json'\n",
    "tso70_path = '/Users/shiprasingh/IIT KGP internship /reasoning-teacher/saved/teacher_completion_data/B_text-davinci-002__C_zs_cot_t70/D_tracking_shuffled_objects.json'\n",
    "\n",
    "# Reading JSON data. The read_json() function returns a dictionary\n",
    "addsub70 = read_json(addsub70_path)\n",
    "coin70 = read_json(coin70_path)\n",
    "du70 = read_json(du70_path)\n",
    "llconc70 = read_json(llconc70_path)\n",
    "mulAr70 = read_json(mulAr70_path)\n",
    "seq70 = read_json(seq70_path)\n",
    "strat70 = read_json(strat70_path)\n",
    "svamp70 = read_json(svamp70_path)\n",
    "tso70 = read_json(tso70_path)\n",
    "\n",
    "# making rationale dictionaries with CORRECT rationales\n",
    "\n",
    "addsub70_Clist = correct_rationale_dict(addsub70)\n",
    "coin70_Clist = correct_rationale_dict(coin70)\n",
    "du70_Clist = correct_rationale_dict(du70)\n",
    "llconc70_Clist = correct_rationale_dict(llconc70)\n",
    "mulAr70_Clist = correct_rationale_dict(mulAr70)\n",
    "seq70_Clist = correct_rationale_dict(seq70)\n",
    "strat70_Clist = correct_rationale_dict(strat70)\n",
    "#svamp70_Clist = correct_rationale_dict(svamp70)\n",
    "tso70_Clist = correct_rationale_dict(tso70)\n",
    "\n",
    "\n",
    "formatTeacherDataset(addsub70_Clist)\n",
    "formatTeacherDataset(coin70_Clist)\n",
    "formatTeacherDataset(du70_Clist)\n",
    "formatTeacherDataset(llconc70_Clist)\n",
    "formatTeacherDataset(mulAr70_Clist)\n",
    "formatTeacherDataset(seq70_Clist)\n",
    "formatTeacherDataset(strat70_Clist)\n",
    "#formatTeacherDataset(svamp70_Clist)\n",
    "formatTeacherDataset(tso70_Clist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b62bf0e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6d43c848",
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing all the student generated rationale JSON\n",
    "\n",
    "add_sub_out = \"/Users/shiprasingh/IIT KGP internship /FLAN Inferences Dataset/addsub_out.json\"\n",
    "coin_flip_out = \"/Users/shiprasingh/IIT KGP internship /FLAN Inferences Dataset/coin_flip_out.json\"\n",
    "date_understanding_out = \"/Users/shiprasingh/IIT KGP internship /FLAN Inferences Dataset/date_understanding_out.json\"\n",
    "last_letter_concatenation_out = \"/Users/shiprasingh/IIT KGP internship /FLAN Inferences Dataset/last_letter_concatenation_out.json\"\n",
    "\n",
    "addsub = read_json(add_sub_out)\n",
    "coin = read_json(coin_flip_out)\n",
    "du = read_json(date_understanding_out)\n",
    "llconc = read_json(last_letter_concatenation_out)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e9205d1",
   "metadata": {},
   "source": [
    "### Reformating student completion dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "db9d2431",
   "metadata": {},
   "outputs": [],
   "source": [
    "removeIncorrectStudentRat(addsub)\n",
    "removeIncorrectStudentRat(coin)\n",
    "removeIncorrectStudentRat(du)\n",
    "removeIncorrectStudentRat(llconc)\n",
    "\n",
    "addSI(addsub, addsub70_Clist)\n",
    "addSI(coin, coin70_Clist)\n",
    "addSI(du, du70_Clist)\n",
    "addSI(llconc, llconc70_Clist)\n",
    "\n",
    "helper(addsub)\n",
    "helper(coin)\n",
    "helper(du)\n",
    "helper(llconc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d93a4666",
   "metadata": {},
   "source": [
    "### Reformating teacher completion dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d53e084a",
   "metadata": {},
   "outputs": [],
   "source": [
    "removeEmpty(addsub70_Clist)\n",
    "removeEmpty(coin70_Clist)\n",
    "removeEmpty(du70_Clist)\n",
    "removeEmpty(llconc70_Clist)\n",
    "\n",
    "addsub70_Clist_new = removeRat(addsub, addsub70_Clist)\n",
    "coin70_Clist_new = removeRat(coin, coin70_Clist)\n",
    "du70_Clist_new = removeRat(du, du70_Clist)\n",
    "llconc70_Clist_new = removeRat(llconc, llconc70_Clist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3c9538ce",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'sample_index': 2, 'question': 'Mary is baking a cake . The recipe wants 8 cups of flour . She already put in 2 cups . How many cups does she need to add ?', 'rationale_dict': [[0, ' \\n\\nThe recipe wants 8 cups of flour.\\nShe already put in 2 cups.\\n\\nThat means she needs to add 8 - 2 = 6 more cups of flour.'], [1, ' \\n\\nMary is baking a cake. \\nThe recipe wants 8 cups of flour. \\nShe already put in 2 cups. \\n\\nThat means she needs to add 6 more cups of flour to the recipe.'], [2, ' \\n\\nMary is baking a cake. \\nThe recipe wants 8 cups of flour. \\nShe already put in 2 cups. \\n\\nSo, she needs to add 6 cups of flour.'], [3, \" \\n\\nFirst, let's figure out how many cups of flour Mary has used so far. We know that she started with 2 cups of flour, and the recipe wants 8 cups of flour. So, we can subtract 2 cups from 8 cups to find out how many more cups Mary needs to add. \\n\\n8 cups - 2 cups = 6 cups \\n\\nSo, Mary needs to add 6 cups of flour to the cake.\"], [4, ' \\nThe recipe wants 8 cups of flour. She already put in 2 cups. \\nThat means she needs to add 8-2=6 more cups of flour.'], [5, ' Mary is baking a cake and the recipe calls for 8 cups of flour. She has already put in 2 cups of flour. So she needs to add 6 more cups of flour.'], [6, ' \\n\\nMary is baking a cake.\\nThe recipe wants 8 cups of flour.\\nShe already put in 2 cups.\\n\\nThat means she needs to add 6 cups more of flour.'], [7, ' \\n\\nMary has already put in 2 cups of flour. \\nThe recipe wants 8 cups of flour. \\nThat means Mary needs to add 6 more cups of flour to the recipe.']]}\n"
     ]
    }
   ],
   "source": [
    "print(addsub70_Clist_new[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "16137943",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n"
     ]
    }
   ],
   "source": [
    "print(type(addsub70_Clist_new[0]['rationale_dict']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3f9e37d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elapsed time: 548.53 seconds\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "s_addsub = MMRcaculation(addsub70_Clist_new, addsub, 0.50, 'bert')\n",
    "end_time = time.time()\n",
    "elapsed_time = end_time - start_time\n",
    "print(f\"Elapsed time: {elapsed_time:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "00f58b64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data has been written to addsub_mmr_flanT5base_bert_L0.50.json\n"
     ]
    }
   ],
   "source": [
    "makeJSON(s_addsub, \"addsub_mmr_flanT5base_bert_L0.50.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "be033e1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elapsed time: 1892.91 seconds\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "s_coin = MMRcaculation(coin70_Clist_new, coin, 0.50, 'bert')\n",
    "end_time = time.time()\n",
    "elapsed_time = end_time - start_time\n",
    "print(f\"Elapsed time: {elapsed_time:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e6ca089b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data has been written to coin_flip_mmr_flanT5base_bert_L0.50.json\n"
     ]
    }
   ],
   "source": [
    "makeJSON(s_coin, \"coin_flip_mmr_flanT5base_bert_L0.50.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "96312cac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elapsed time: 2123.96 seconds\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "s_du = MMRcaculation(du70_Clist_new, du, 0.50, 'bert')\n",
    "end_time = time.time()\n",
    "elapsed_time = end_time - start_time\n",
    "print(f\"Elapsed time: {elapsed_time:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "4f7db68d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data has been written to date_understanding_mmr_flanT5base_bert_L0.50.json\n"
     ]
    }
   ],
   "source": [
    "makeJSON(s_du, \"date_understanding_mmr_flanT5base_bert_L0.50.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "dee207d0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elapsed time: 0.00 seconds\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "s_llconc = MMRcaculation(llconc70_Clist_new, llconc, 0.50, 'bert')\n",
    "end_time = time.time()\n",
    "elapsed_time = end_time - start_time\n",
    "print(f\"Elapsed time: {elapsed_time:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f9a21d0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data has been written to Last_Letter_Concatenation_mmr_flanT5base_bert_L0.50.json\n"
     ]
    }
   ],
   "source": [
    "makeJSON(s_llconc, \"Last_Letter_Concatenation_mmr_flanT5base_bert_L0.50.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "bf3344a5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nstart_time = time.time()\\ns_mulAr = MMRcaculation(mulAr70_Clist)\\nend_time = time.time()\\nelapsed_time = end_time - start_time\\nprint(f\"Elapsed time: {elapsed_time:.2f} seconds\")\\n\\nmakeJSON(s_mulAr, \"MultiArith_mmr_q_rL.json\")\\n'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "start_time = time.time()\n",
    "s_mulAr = MMRcaculation(mulAr70_Clist)\n",
    "end_time = time.time()\n",
    "elapsed_time = end_time - start_time\n",
    "print(f\"Elapsed time: {elapsed_time:.2f} seconds\")\n",
    "\n",
    "makeJSON(s_mulAr, \"MultiArith_mmr_q_rL.json\")\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "4e4f1fe8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ndef pairWiseScore(df, rationale_list):\\n    for i in range(len(rationale_list)):\\n        for j in range(len(rationale_list)):\\n            if i == j:\\n                df.iloc[i, j] = 1.0\\n            else:\\n                score = calculate_rouge(rationale_list[i], rationale_list[j])\\n                df.iloc[i, j] = score[\\'rouge2\\'].fmeasure        # stores only ROUGE2 fmeasure (for now at least)\\n                \\n                \\nrationales = [\\'ci_0\\', \\'ci_1\\', \\'ci_2\\', \\'ci_3\\', \\'ci_4\\', \\'ci_5\\', \\'ci_6\\', \\'ci_7\\']\\ndf = pd.DataFrame(index=rationales, columns=rationales)\\npairWiseScore(df, rationale_list)\\n\\n\\ndef quesScore(question, rationale_list):\\n    d = dict()\\n    for i in range(len(rationale_list)):\\n        score = calculate_rouge(question, rationale_list[i])\\n        ci = \"ci_\" + str(i)\\n        d[ci] = score[\\'rouge2\\'].fmeasure\\n    return d\\n    \\n\\nques_sim_list = quesScore(question, rationale_list)\\nprint(ques_sim_list)\\n\\n\\n# S contains the rationale with the maximum similarity score with the question \\n# R contains the rationales other than the ones in S\\nmaxi = max(ques_sim_list.values())\\nS = [(key, value) for key, value in ques_sim_list.items() if value == maxi]\\nR = [(key, value) for key, value in ques_sim_list.items() if (key, value) not in S]\\n'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "def pairWiseScore(df, rationale_list):\n",
    "    for i in range(len(rationale_list)):\n",
    "        for j in range(len(rationale_list)):\n",
    "            if i == j:\n",
    "                df.iloc[i, j] = 1.0\n",
    "            else:\n",
    "                score = calculate_rouge(rationale_list[i], rationale_list[j])\n",
    "                df.iloc[i, j] = score['rouge2'].fmeasure        # stores only ROUGE2 fmeasure (for now at least)\n",
    "                \n",
    "                \n",
    "rationales = ['ci_0', 'ci_1', 'ci_2', 'ci_3', 'ci_4', 'ci_5', 'ci_6', 'ci_7']\n",
    "df = pd.DataFrame(index=rationales, columns=rationales)\n",
    "pairWiseScore(df, rationale_list)\n",
    "\n",
    "\n",
    "def quesScore(question, rationale_list):\n",
    "    d = dict()\n",
    "    for i in range(len(rationale_list)):\n",
    "        score = calculate_rouge(question, rationale_list[i])\n",
    "        ci = \"ci_\" + str(i)\n",
    "        d[ci] = score['rouge2'].fmeasure\n",
    "    return d\n",
    "    \n",
    "\n",
    "ques_sim_list = quesScore(question, rationale_list)\n",
    "print(ques_sim_list)\n",
    "\n",
    "\n",
    "# S contains the rationale with the maximum similarity score with the question \n",
    "# R contains the rationales other than the ones in S\n",
    "maxi = max(ques_sim_list.values())\n",
    "S = [(key, value) for key, value in ques_sim_list.items() if value == maxi]\n",
    "R = [(key, value) for key, value in ques_sim_list.items() if (key, value) not in S]\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5f603e6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9356c86",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
